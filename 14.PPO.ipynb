{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.hyperparameters import Config\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# meta info\n",
    "config.algo = 'a2c'\n",
    "config.env_id = 'BreakoutNoFrameskip-v4'\n",
    "config.seed = None\n",
    "config.inference = False\n",
    "config.print_threshold = 100\n",
    "config.save_threshold = 1000\n",
    "config.render = False\n",
    "\n",
    "# preprocessing\n",
    "config.stack_frames = 4\n",
    "config.adaptive_repeat = [4]\n",
    "config.s_norm = 255.0\n",
    "config.sticky_actions = 0.0\n",
    "\n",
    "# Learning Control Variables\n",
    "config.max_tsteps  = int(1e7)\n",
    "config.learn_start = 0\n",
    "config.num_envs    = 16\n",
    "config.update_freq = 5\n",
    "config.lr          = 7e-4\n",
    "config.use_lr_schedule = True\n",
    "config.grad_norm_max = 0.5\n",
    "config.gamma = 0.99\n",
    "\n",
    "# RMSProp params\n",
    "config.rms_alpha = 0.99\n",
    "config.rms_eps = 1e-5\n",
    "\n",
    "#Recurrent control\n",
    "config.policy_gradient_recurrent_policy = False\n",
    "config.gru_size = 512\n",
    "\n",
    "# A2C Controls\n",
    "config.entropy_loss_weight=0.01\n",
    "config.value_loss_weight=0.5\n",
    "\n",
    "# GAE Controls\n",
    "config.use_gae = True\n",
    "config.gae_tau = 0.95\n",
    "\n",
    "#PPO controls\n",
    "self.ppo_epoch = 3\n",
    "self.ppo_mini_batch = 32\n",
    "self.ppo_clip_param = 0.1\n",
    "self.use_ppo_vf_clip = False\n",
    "self.anneal_ppo_clip = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.RolloutStorage import RolloutStorage as Storage\n",
    "\n",
    "class RolloutStorage(Storage):\n",
    "    def __init__(self, num_steps, num_processes, obs_shape, action_space, state_size, device, USE_GAE=True, gae_tau=0.95):\n",
    "        super(RolloutStorage, self).__init__(num_steps, num_processes, obs_shape, action_space, state_size, device, USE_GAE, gae_tau)\n",
    "\n",
    "    def feed_forward_generator(self, advantages, num_mini_batch):\n",
    "        num_steps, num_processes = self.rewards.size()[0:2]\n",
    "        batch_size = num_processes * num_steps\n",
    "        assert batch_size >= num_mini_batch, (\n",
    "            f\"PPO requires the number processes ({num_processes}) \"\n",
    "            f\"* number of steps ({num_steps}) = {num_processes * num_steps} \"\n",
    "            f\"to be greater than or equal to the number of PPO mini batches ({num_mini_batch}).\")\n",
    "        mini_batch_size = batch_size // num_mini_batch\n",
    "        sampler = BatchSampler(SubsetRandomSampler(range(batch_size)), mini_batch_size, drop_last=False)\n",
    "        for indices in sampler:\n",
    "            observations_batch = self.observations[:-1].view(-1,\n",
    "                                        *self.observations.size()[2:])[indices]\n",
    "            states_batch = self.states[:-1].view(-1, self.states.size(-1))[indices]\n",
    "            actions_batch = self.actions.view(-1, self.actions.size(-1))[indices]\n",
    "            value_preds_batch = self.value_preds[:-1].view(-1, 1)[indices]\n",
    "            return_batch = self.returns[:-1].view(-1, 1)[indices]\n",
    "            masks_batch = self.masks[:-1].view(-1, 1)[indices]\n",
    "            old_action_log_probs_batch = self.action_log_probs.view(-1, 1)[indices]\n",
    "            adv_targ = advantages.view(-1, 1)[indices]\n",
    "\n",
    "            yield observations_batch, states_batch, \\\n",
    "                actions_batch, value_preds_batch, return_batch, masks_batch, \\\n",
    "                old_action_log_probs_batch, adv_targ\n",
    "\n",
    "    def recurrent_generator(self, advantages, num_mini_batch):\n",
    "        num_processes = self.rewards.size(1)\n",
    "        assert num_processes >= num_mini_batch, (\n",
    "            f\"PPO requires the number processes ({num_processes}) \"\n",
    "            f\"to be greater than or equal to the number of PPO mini batches ({num_mini_batch}).\")\n",
    "        num_envs_per_batch = num_processes // num_mini_batch\n",
    "        perm = torch.randperm(num_processes)\n",
    "        for start_ind in range(0, num_processes, num_envs_per_batch):\n",
    "            observations_batch = []\n",
    "            states_batch = []\n",
    "            actions_batch = []\n",
    "            value_preds_batch = []\n",
    "            return_batch = []\n",
    "            masks_batch = []\n",
    "            old_action_log_probs_batch = []\n",
    "            adv_targ = []\n",
    "\n",
    "            for offset in range(num_envs_per_batch):\n",
    "                ind = perm[start_ind + offset]\n",
    "                observations_batch.append(self.observations[:-1, ind])\n",
    "                states_batch.append(self.states[0:1, ind])\n",
    "                actions_batch.append(self.actions[:, ind])\n",
    "                value_preds_batch.append(self.value_preds[:-1, ind])\n",
    "                return_batch.append(self.returns[:-1, ind])\n",
    "                masks_batch.append(self.masks[:-1, ind])\n",
    "                old_action_log_probs_batch.append(self.action_log_probs[:, ind])\n",
    "                adv_targ.append(advantages[:, ind])\n",
    "\n",
    "            T, N = self.num_steps, num_envs_per_batch\n",
    "            # These are all tensors of size (T, N, -1)\n",
    "            observations_batch = torch.stack(observations_batch, 1)\n",
    "            actions_batch = torch.stack(actions_batch, 1)\n",
    "            value_preds_batch = torch.stack(value_preds_batch, 1)\n",
    "            return_batch = torch.stack(return_batch, 1)\n",
    "            masks_batch = torch.stack(masks_batch, 1)\n",
    "            old_action_log_probs_batch = torch.stack(old_action_log_probs_batch, 1)\n",
    "            adv_targ = torch.stack(adv_targ, 1)\n",
    "\n",
    "            # States is just a (N, -1) tensor\n",
    "            states_batch = torch.stack(states_batch, 1).view(N, -1)\n",
    "\n",
    "            # Flatten the (T, N, ...) tensors to (T * N, ...)\n",
    "            observations_batch = _flatten_helper(T, N, observations_batch)\n",
    "            actions_batch = _flatten_helper(T, N, actions_batch)\n",
    "            value_preds_batch = _flatten_helper(T, N, value_preds_batch)\n",
    "            return_batch = _flatten_helper(T, N, return_batch)\n",
    "            masks_batch = _flatten_helper(T, N, masks_batch)\n",
    "            old_action_log_probs_batch = _flatten_helper(T, N, \\\n",
    "                    old_action_log_probs_batch)\n",
    "            adv_targ = _flatten_helper(T, N, adv_targ)\n",
    "\n",
    "            yield observations_batch, states_batch, \\\n",
    "                actions_batch, value_preds_batch, return_batch, masks_batch, \\\n",
    "                old_action_log_probs_batch, adv_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from agents.A2C import Agent as A2C\n",
    "from utils import LinearSchedule\n",
    "\n",
    "class Agent(A2C):\n",
    "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym', tb_writer=None):\n",
    "        super(Agent, self).__init__(static_policy, env, config, log_dir, tb_writer=tb_writer)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.lr, eps=self.config.adam_eps)\n",
    "        \n",
    "        if self.config.anneal_ppo_clip:\n",
    "            self.anneal_clip_param_fun = LinearSchedule(self.config.ppo_clip_param, 0.0, 1.0, config.max_tsteps)\n",
    "        else:\n",
    "            self.anneal_clip_param_fun = LinearSchedule(self.config.ppo_clip_param, None, 1.0, config.max_tsteps)\n",
    "            \n",
    "        self.rollouts = RolloutStorage(self.config.update_freq , self.config.num_envs,\n",
    "            self.num_feats, self.envs.action_space, self.model.state_size,\n",
    "            self.config.device, config.use_gae, config.gae_tau)\n",
    "\n",
    "    def compute_loss(self, sample, next_value, clip_param):\n",
    "        observations_batch, states_batch, actions_batch, value_preds_batch, return_batch, masks_batch, old_action_log_probs_batch, adv_targ = sample\n",
    "\n",
    "        values, action_log_probs, dist_entropy, states = self.evaluate_actions(observations_batch,\n",
    "                                                            actions_batch,\n",
    "                                                            states_batch,\n",
    "                                                            masks_batch)\n",
    "\n",
    "        ratio = torch.exp(action_log_probs - old_action_log_probs_batch)\n",
    "        surr1 = ratio * adv_targ\n",
    "        surr2 = torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param) * adv_targ\n",
    "        action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        if self.config.use_ppo_vf_clip:\n",
    "            value_pred_clipped = value_preds_batch + (values - value_preds_batch).clamp(-clip_param, clip_param)\n",
    "            value_losses = (values - return_batch).pow(2)\n",
    "            value_losses_clipped = (value_pred_clipped - return_batch).pow(2)\n",
    "            value_loss = torch.max(value_losses, value_losses_clipped).mul(0.5).mean()\n",
    "        else:\n",
    "            value_loss = (return_batch - values).pow(2).mul(0.5).mean()\n",
    "\n",
    "        loss = action_loss + self.config.value_loss_weight * value_loss\n",
    "        loss -= self.config.entropy_loss_weight * dist_entropy\n",
    "\n",
    "        return loss, action_loss, value_loss, dist_entropy\n",
    "\n",
    "    def update_(self, rollout, next_value, tstep):\n",
    "        rollout.compute_returns(next_value, self.config.gamma)\n",
    "\n",
    "        advantages = rollout.returns[:-1] - rollout.value_preds[:-1]\n",
    "        advantages = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-5)\n",
    "\n",
    "        value_loss_epoch = 0\n",
    "        action_loss_epoch = 0\n",
    "        dist_entropy_epoch = 0\n",
    "\n",
    "        clip_param = self.anneal_clip_param_fun(tstep)\n",
    "\n",
    "        all_grad_norms = []\n",
    "        all_sigma_norms = []\n",
    "\n",
    "        for e in range(self.config.ppo_epoch):\n",
    "            if self.model.use_gru:\n",
    "                data_generator = rollout.recurrent_generator(\n",
    "                    advantages, self.config.ppo_mini_batch)\n",
    "            else:\n",
    "                data_generator = rollout.feed_forward_generator(\n",
    "                    advantages, self.config.ppo_mini_batch)\n",
    "\n",
    "\n",
    "            for sample in data_generator:\n",
    "                loss, action_loss, value_loss, dist_entropy = self.compute_loss(sample, next_value, clip_param)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_norm_max)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    grad_norm = 0.\n",
    "                    for p in self.model.parameters():\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        grad_norm += param_norm.item() ** 2\n",
    "                    grad_norm = grad_norm ** (1./2.)\n",
    "                    all_grad_norms.append(grad_norm)\n",
    "\n",
    "                    if self.config.noisy_nets:\n",
    "                        sigma_norm = 0.\n",
    "                        for name, p in self.model.named_parameters():\n",
    "                            if p.requires_grad and 'sigma' in name:\n",
    "                                param_norm = p.data.norm(2)\n",
    "                                sigma_norm += param_norm.item() ** 2\n",
    "                        sigma_norm = sigma_norm ** (1./2.)\n",
    "                        all_sigma_norms.append(sigma_norm)\n",
    "\n",
    "                value_loss_epoch += value_loss.item()\n",
    "                action_loss_epoch += action_loss.item()\n",
    "                dist_entropy_epoch += dist_entropy.item()\n",
    "        \n",
    "        value_loss_epoch /= (self.config.ppo_epoch * self.config.ppo_mini_batch)\n",
    "        action_loss_epoch /= (self.config.ppo_epoch * self.config.ppo_mini_batch)\n",
    "        dist_entropy_epoch /= (self.config.ppo_epoch * self.config.ppo_mini_batch)\n",
    "        total_loss = value_loss_epoch + action_loss_epoch + dist_entropy_epoch\n",
    "\n",
    "        self.tb_writer.add_scalar('Loss/Total Loss', total_loss, tstep)\n",
    "        self.tb_writer.add_scalar('Loss/Policy Loss', action_loss_epoch, tstep)\n",
    "        self.tb_writer.add_scalar('Loss/Value Loss', value_loss_epoch, tstep)\n",
    "        self.tb_writer.add_scalar('Loss/Forward Dynamics Loss', 0., tstep)\n",
    "        self.tb_writer.add_scalar('Loss/Inverse Dynamics Loss', 0., tstep)\n",
    "        self.tb_writer.add_scalar('Policy/Entropy', dist_entropy_epoch, tstep)\n",
    "        self.tb_writer.add_scalar('Policy/Value Estimate', 0, tstep)\n",
    "        if all_sigma_norms:\n",
    "            self.tb_writer.add_scalar('Policy/Sigma Norm', np.mean(all_sigma_norms), tstep)\n",
    "        self.tb_writer.add_scalar('Learning/Learning Rate', np.mean([param_group['lr'] for param_group in self.optimizer.param_groups]), tstep)\n",
    "        self.tb_writer.add_scalar('Learning/Grad Norm', np.mean(all_grad_norms), tstep)\n",
    "\n",
    "        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch, 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import train \n",
    "%matplotlib inline\n",
    "\n",
    "train(config, Agent, ipynb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
