{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network with Noisy Networks for Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import math\n",
    "\n",
    "from utils.wrappers import *\n",
    "from utils.ReplayMemory import ExperienceReplayMemory\n",
    "from networks.layers import NoisyLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#epsilon variables\n",
    "SIGMA_INIT=0.5\n",
    "\n",
    "#misc agent variables\n",
    "GAMMA=0.99\n",
    "LR=1e-4\n",
    "\n",
    "#memory\n",
    "TARGET_NET_UPDATE_FREQ = 1000\n",
    "EXP_REPLAY_SIZE = 100000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Learning control variables\n",
    "LEARN_START = 10000\n",
    "MAX_FRAMES=700000\n",
    "\n",
    "#Nstep controls\n",
    "N_STEPS=1\n",
    "\n",
    "#Categorical Params\n",
    "ATOMS = 51\n",
    "V_MAX = 10\n",
    "V_MIN = -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions, sigma_init=0.5, atoms=51):\n",
    "        super(CategoricalDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.atoms = atoms\n",
    "\n",
    "        self.conv1 = nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        self.fc1 = NoisyLinear(self.feature_size(), 512, sigma_init)\n",
    "        self.fc2 = NoisyLinear(512, self.num_actions*self.atoms, sigma_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x.view(-1, self.num_actions, self.atoms), dim=2)\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.conv3(self.conv2(self.conv1(torch.zeros(1, *self.input_shape)))).view(1, -1).size(1)\n",
    "\n",
    "    def sample_noise(self):\n",
    "        self.fc1.sample_noise()\n",
    "        self.fc2.sample_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, static_policy=False, env=None):\n",
    "        super(Model, self).__init__()\n",
    "        self.gamma=GAMMA\n",
    "        self.lr = LR\n",
    "        self.target_net_update_freq = TARGET_NET_UPDATE_FREQ\n",
    "        self.experience_replay_size = EXP_REPLAY_SIZE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.learn_start = LEARN_START\n",
    "        self.sigma_init = SIGMA_INIT\n",
    "        self.atoms=ATOMS\n",
    "        self.v_max=V_MAX\n",
    "        self.v_min=V_MIN\n",
    "        self.supports = torch.linspace(self.v_min, self.v_max, self.atoms).view(1, 1, self.atoms).to(device)\n",
    "        self.delta = (self.v_max - self.v_min) / (self.atoms - 1)\n",
    "        \n",
    "        self.num_actions = env.action_space.n\n",
    "        self.env = env\n",
    "\n",
    "        self.static_policy=static_policy\n",
    "        \n",
    "        self.declare_networks()\n",
    "            \n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        #move to correct device\n",
    "        self.model = self.model.to(device)\n",
    "        self.target_model.to(device)\n",
    "\n",
    "        if self.static_policy:\n",
    "            self.model.eval()\n",
    "            self.target_model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            self.target_model.train()\n",
    "\n",
    "        self.update_count = 0\n",
    "\n",
    "        self.memory = ExperienceReplayMemory(self.experience_replay_size)\n",
    "\n",
    "        self.nsteps = N_STEPS\n",
    "        self.nstep_buffer = []\n",
    "    \n",
    "    \n",
    "    def declare_networks(self):\n",
    "        self.model = CategoricalDQN(self.env.observation_space.shape, self.env.action_space.n, self.sigma_init, atoms=self.atoms)\n",
    "        self.target_model = CategoricalDQN(self.env.observation_space.shape, self.env.action_space.n, self.sigma_init, atoms=self.atoms)\n",
    "\n",
    "        \n",
    "    def append_to_replay(self, s, a, r, s_):\n",
    "        self.nstep_buffer.append((s, a, r, s_))\n",
    "\n",
    "        if(len(self.nstep_buffer)<self.nsteps):\n",
    "            return\n",
    "        \n",
    "        R = sum([self.nstep_buffer[i][2]*(self.gamma**i) for i in range(self.nsteps)])\n",
    "        state, action, _, _ = self.nstep_buffer.pop(0)\n",
    "\n",
    "        self.memory.push((state, action, R, s_))\n",
    "\n",
    "\n",
    "    def prep_minibatch(self):\n",
    "        # random transition batch is taken from experience replay memory\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        \n",
    "        batch_state, batch_action, batch_reward, batch_next_state = zip(*transitions)\n",
    "\n",
    "        shape = (-1,)+self.env.observation_space.shape\n",
    "\n",
    "        batch_state = torch.tensor(batch_state, device=device, dtype=torch.float).view(shape)\n",
    "        batch_action = torch.tensor(batch_action, device=device, dtype=torch.long).squeeze().view(-1, 1)\n",
    "        batch_reward = torch.tensor(batch_reward, device=device, dtype=torch.float).squeeze().view(-1, 1)\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch_next_state)), device=device, dtype=torch.uint8)\n",
    "        try: #sometimes all next states are false\n",
    "            non_final_next_states = torch.tensor([s for s in batch_next_state if s is not None], device=device, dtype=torch.float).view(shape)\n",
    "            empty_next_state_values = False\n",
    "        except:\n",
    "            non_final_next_states = None\n",
    "            empty_next_state_values = True\n",
    "\n",
    "        return batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values\n",
    "\n",
    "    def projection_distribution(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values = batch_vars\n",
    "\n",
    "        with torch.no_grad():\n",
    "            max_next_dist = torch.zeros((self.batch_size, 1, self.atoms), device=device, dtype=torch.float) + 1./self.atoms\n",
    "            if not empty_next_state_values:\n",
    "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
    "                self.target_model.sample_noise()\n",
    "                max_next_dist[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
    "                max_next_dist = max_next_dist.squeeze()\n",
    "\n",
    "\n",
    "            Tz = batch_reward.view(-1, 1) + (self.gamma**self.nsteps)*self.supports.view(1, -1) * non_final_mask.to(torch.float).view(-1, 1)\n",
    "            Tz = Tz.clamp(self.v_min, self.v_max)\n",
    "            b = (Tz - self.v_min) / self.delta\n",
    "            l = b.floor().to(torch.int64)\n",
    "            u = b.ceil().to(torch.int64)\n",
    "            l[(u > 0) * (l == u)] -= 1\n",
    "            u[(l < (self.atoms - 1)) * (l == u)] += 1\n",
    "            \n",
    "\n",
    "            offset = torch.linspace(0, (self.batch_size - 1) * self.atoms, self.batch_size).unsqueeze(dim=1).expand(self.batch_size, self.atoms).to(batch_action)\n",
    "            m = batch_state.new_zeros(self.batch_size, self.atoms)\n",
    "            m.view(-1).index_add_(0, (l + offset).view(-1), (max_next_dist * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n",
    "            m.view(-1).index_add_(0, (u + offset).view(-1), (max_next_dist * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n",
    "\n",
    "        return m\n",
    "    \n",
    "    def compute_loss(self, batch_vars):\n",
    "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values = batch_vars\n",
    "\n",
    "        batch_action = batch_action.unsqueeze(dim=-1).expand(-1, -1, self.atoms)\n",
    "        batch_reward = batch_reward.view(-1, 1, 1)\n",
    "\n",
    "        #estimate\n",
    "        self.model.sample_noise()\n",
    "        current_dist = self.model(batch_state).gather(1, batch_action).squeeze()\n",
    "\n",
    "        target_prob = self.projection_distribution(batch_vars)\n",
    "          \n",
    "        loss = -(target_prob * current_dist.log()).sum(-1)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, s, a, r, s_, frame=0):\n",
    "        if self.static_policy:\n",
    "            return None\n",
    "\n",
    "        self.append_to_replay(s, a, r, s_)\n",
    "\n",
    "        if frame < self.learn_start:\n",
    "            return None\n",
    "\n",
    "        batch_vars = self.prep_minibatch()\n",
    "\n",
    "        loss = self.compute_loss(batch_vars)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        '''for param in self.model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)'''\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_target_model()\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def get_action(self, s):\n",
    "        with torch.no_grad():\n",
    "            X = torch.tensor([s], device=device, dtype=torch.float)\n",
    "            self.model.sample_noise()\n",
    "            a = self.model(X) * self.supports\n",
    "            a = a.sum(dim=2).max(1)[1].view(1, 1)\n",
    "            return a.item()\n",
    "            \n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.update_count+=1\n",
    "        self.update_count = self.update_count % self.target_net_update_freq\n",
    "        if self.update_count == 0:\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_max_next_state_action(self, next_states):\n",
    "        next_dist = self.target_model(next_states) * self.supports\n",
    "        return next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(-1, -1, self.atoms)\n",
    "\n",
    "    def finish_nstep(self):\n",
    "        while len(self.nstep_buffer) > 0:\n",
    "            R = sum([self.nstep_buffer[i][2]*(self.gamma**i) for i in range(len(self.nstep_buffer))])\n",
    "            state, action, _, _ = self.nstep_buffer.pop(0)\n",
    "\n",
    "            state = [state]\n",
    "            action = [[action]]\n",
    "            reward = [[R]]\n",
    "\n",
    "            self.memory.push((state, action, reward, None))\n",
    "\n",
    "    def huber(self, x):\n",
    "        cond = (x < 1.0).float().detach()\n",
    "        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1.0 - cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses, elapsed_time):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 8, got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-96ebddf2f1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4d62f09c5944>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s, a, r, s_, frame)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mbatch_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4d62f09c5944>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, batch_vars)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mbatch_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_final_next_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_final_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty_next_state_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mbatch_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_action\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 8, got 6)"
     ]
    }
   ],
   "source": [
    "start=timer()\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env, frame_stack=True)\n",
    "env    = wrap_pytorch(env)\n",
    "model = Model(env=env)\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "observation = env.reset()\n",
    "for frame_idx in range(1, MAX_FRAMES + 1):\n",
    "    action = model.get_action(observation)\n",
    "    prev_observation=observation\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    loss = model.update(prev_observation, action, reward, observation, frame_idx)\n",
    "    episode_reward += reward\n",
    "\n",
    "    if done:\n",
    "        model.finish_nstep()\n",
    "        observation = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "        if np.mean(all_rewards[-10:]) > 19:\n",
    "            plot(frame_idx, all_rewards, losses, timedelta(seconds=int(timer()-start)))\n",
    "            break\n",
    "\n",
    "    if loss is not None:\n",
    "        losses.append(loss)\n",
    "\n",
    "    if frame_idx % 10000 == 0:\n",
    "            plot(frame_idx, all_rewards, losses, timedelta(seconds=int(timer()-start)))\n",
    "\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
